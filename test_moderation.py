"""
Script de test pour la modÃ©ration de contenu
"""
from content_moderation import (
    detect_inappropriate_content, 
    validate_image_for_clothing,
    ContentModerationError
)
from PIL import Image
import io
import numpy as np

def create_test_image(skin_percentage=0.3, brightness=128):
    """
    CrÃ©e une image de test avec un pourcentage de peau contrÃ´lÃ©
    """
    # CrÃ©er une image 224x224
    img = np.zeros((224, 224, 3), dtype=np.uint8)
    
    # Remplir avec une couleur de base
    img[:, :] = [50, 50, 100]  # Bleu foncÃ© (fond)
    
    # Ajouter de la "peau" (couleur chair)
    if skin_percentage > 0:
        num_skin_pixels = int(224 * 224 * skin_percentage)
        skin_color = [220, 180, 150]  # Couleur chair
        
        # Remplir alÃ©atoirement des pixels avec de la couleur peau
        indices = np.random.choice(224 * 224, num_skin_pixels, replace=False)
        for idx in indices:
            y = idx // 224
            x = idx % 224
            img[y, x] = skin_color
    
    # Convertir en image PIL
    pil_img = Image.fromarray(img, 'RGB')
    
    # Convertir en bytes
    img_bytes = io.BytesIO()
    pil_img.save(img_bytes, format='JPEG')
    return img_bytes.getvalue()

def test_safe_image():
    """Test avec une image sÃ»re (peu de peau)"""
    print("\n" + "="*60)
    print("ğŸ§ª Test 1: Image sÃ»re (20% peau)")
    print("="*60)
    
    image_bytes = create_test_image(skin_percentage=0.2)
    
    try:
        result = detect_inappropriate_content(image_bytes)
        print(f"âœ… RÃ©sultat: {'SAFE' if result['is_safe'] else 'UNSAFE'}")
        print(f"   Pourcentage de peau: {result['skin_percentage']:.1%}")
        print(f"   LuminositÃ©: {result['brightness']:.1f}")
        if result['reasons']:
            print(f"   Raisons: {', '.join(result['reasons'])}")
    except ContentModerationError as e:
        print(f"âŒ BLOQUÃ‰: {e.message}")
        print(f"   Raison: {e.reason}")

def test_nsfw_image():
    """Test avec une image NSFW (beaucoup de peau)"""
    print("\n" + "="*60)
    print("ğŸ§ª Test 2: Image inappropriÃ©e (80% peau)")
    print("="*60)
    
    image_bytes = create_test_image(skin_percentage=0.8)
    
    try:
        result = detect_inappropriate_content(image_bytes)
        print(f"âœ… RÃ©sultat: {'SAFE' if result['is_safe'] else 'UNSAFE'}")
        print(f"   Pourcentage de peau: {result['skin_percentage']:.1%}")
        print(f"   LuminositÃ©: {result['brightness']:.1f}")
        if result['reasons']:
            print(f"   Raisons: {', '.join(result['reasons'])}")
    except ContentModerationError as e:
        print(f"âŒ BLOQUÃ‰: {e.message}")
        print(f"   Raison: {e.reason}")
        print(f"   Confiance: {e.confidence:.1%}")

def test_borderline_image():
    """Test avec une image limite (au seuil)"""
    print("\n" + "="*60)
    print("ğŸ§ª Test 3: Image limite (60% peau)")
    print("="*60)
    
    image_bytes = create_test_image(skin_percentage=0.6)
    
    try:
        result = detect_inappropriate_content(image_bytes)
        print(f"âœ… RÃ©sultat: {'SAFE' if result['is_safe'] else 'UNSAFE'}")
        print(f"   Pourcentage de peau: {result['skin_percentage']:.1%}")
        print(f"   LuminositÃ©: {result['brightness']:.1f}")
        if result['reasons']:
            print(f"   Raisons: {', '.join(result['reasons'])}")
    except ContentModerationError as e:
        print(f"âŒ BLOQUÃ‰: {e.message}")
        print(f"   Raison: {e.reason}")
        print(f"   Confiance: {e.confidence:.1%}")

def test_validation():
    """Test de la validation complÃ¨te"""
    print("\n" + "="*60)
    print("ğŸ§ª Test 4: Validation complÃ¨te")
    print("="*60)
    
    # Test image valide
    print("\nğŸ“¸ Image valide:")
    try:
        image_bytes = create_test_image(skin_percentage=0.15)
        result = validate_image_for_clothing(image_bytes)
        print(f"   âœ… Image acceptÃ©e")
        print(f"   VÃ©rification: {result['is_safe']}")
    except ContentModerationError as e:
        print(f"   âŒ Image rejetÃ©e: {e.reason}")
    except ValueError as e:
        print(f"   âŒ Image invalide: {str(e)}")
    
    # Test image inappropriÃ©e
    print("\nğŸ“¸ Image inappropriÃ©e:")
    try:
        image_bytes = create_test_image(skin_percentage=0.75)
        result = validate_image_for_clothing(image_bytes)
        print(f"   âœ… Image acceptÃ©e (ne devrait pas arriver)")
    except ContentModerationError as e:
        print(f"   âŒ Image rejetÃ©e: {e.reason}")
        print(f"   âœ… ModÃ©ration fonctionne correctement!")
    except ValueError as e:
        print(f"   âŒ Image invalide: {str(e)}")

def main():
    """ExÃ©cuter tous les tests"""
    print("\n" + "ğŸ›¡ï¸ " * 20)
    print("TESTS DE MODÃ‰RATION DE CONTENU")
    print("ğŸ›¡ï¸ " * 20)
    
    test_safe_image()
    test_nsfw_image()
    test_borderline_image()
    test_validation()
    
    print("\n" + "="*60)
    print("ğŸ“Š RÃ‰SUMÃ‰")
    print("="*60)
    print("""
âœ… La modÃ©ration de contenu est active
ğŸ›¡ï¸  Les images avec >60% de peau sont bloquÃ©es
âš ï¸  HTTP 451 est retournÃ© pour contenu inappropriÃ©
ğŸ“¸ Les images de vÃªtements normales passent sans problÃ¨me
    """)
    
    print("ğŸ’¡ NOTES:")
    print("   - Le seuil NSFW peut Ãªtre ajustÃ© dans config.py")
    print("   - La dÃ©tection utilise une heuristique de couleur de peau")
    print("   - Pour MVP: dÃ©tection basique mais efficace")
    print("   - Pour production: envisager un modÃ¨le ML spÃ©cialisÃ© (NudeNet)")

if __name__ == "__main__":
    main()
