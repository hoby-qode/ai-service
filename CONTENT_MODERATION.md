# üõ°Ô∏è Mod√©ration de Contenu - AI Service

## Vue d'ensemble

Le service AI inclut maintenant un syst√®me de **mod√©ration de contenu** pour bloquer automatiquement les images inappropri√©es (nudit√©, contenu sexuel, etc.).

## ‚úÖ Fonctionnalit√©s

### Protection Automatique

- ‚úÖ D√©tection de nudit√©
- ‚úÖ D√©tection de contenu sexuel
- ‚úÖ Analyse du pourcentage de peau visible
- ‚úÖ V√©rification des dimensions et formats suspects
- ‚úÖ Blocage automatique avec code d'erreur HTTP 451

### Configuration

La mod√©ration est configurable via `config.py` :

```python
CONTENT_MODERATION_CONFIG = {
    "enabled": True,              # Activer/d√©sactiver
    "nsfw_threshold": 0.6,        # Seuil de d√©tection (0-1)
    "block_nsfw": True,           # Bloquer les images NSFW
    "block_violence": True,       # Bloquer les images violentes
}
```

## üîç Comment √ßa fonctionne

### Algorithme de D√©tection

1. **Analyse de couleur de peau** (heuristique RGB)
   - D√©tecte les pixels correspondant √† la couleur chair
   - Calcule le pourcentage de peau dans l'image
2. **V√©rification de luminosit√©**
   - Images trop claires ou trop sombres suspectes
3. **Analyse de composition**

   - Ratio largeur/hauteur
   - Concentration de peau

4. **D√©cision**
   - Si pourcentage de peau > seuil ‚Üí BLOQU√â
   - Sinon ‚Üí ACCEPT√â

### Seuils par D√©faut

| Seuil            | Valeur     | Description              |
| ---------------- | ---------- | ------------------------ |
| `nsfw_threshold` | 0.6 (60%)  | Si >60% de peau ‚Üí bloqu√© |
| Borderline       | 0.4-0.5    | Zone d'alerte            |
| Safe             | <0.4 (40%) | Image consid√©r√©e s√ªre    |

## üì° Codes de R√©ponse HTTP

### ‚úÖ 200 OK - Image Accept√©e

```json
{
  "name": "Coton noir",
  "type": "haut",
  "color": "noir",
  ...
  "moderation": {
    "is_safe": true,
    "checked": true
  }
}
```

### ‚ùå 451 Unavailable For Legal Reasons - Contenu Bloqu√©

```json
{
  "detail": {
    "error": "content_blocked",
    "message": "Image refus√©e : contenu inappropri√© d√©tect√©",
    "reason": "nudit√© d√©tect√©e",
    "confidence": 0.78,
    "help": "Veuillez uploader une image de v√™tement appropri√©e..."
  }
}
```

### ‚ùå 400 Bad Request - Image Invalide

```json
{
  "detail": {
    "error": "invalid_image",
    "message": "Image trop petite. Minimum 50x50 pixels requis."
  }
}
```

## üß™ Tests

### Tester la Mod√©ration

```bash
cd ai-service
python test_moderation.py
```

### Exemple de Test Manuel

```python
from content_moderation import validate_image_for_clothing

# Charger une image
with open('image.jpg', 'rb') as f:
    image_bytes = f.read()

# Valider
try:
    result = validate_image_for_clothing(image_bytes)
    print(f"‚úÖ Image accept√©e: {result['is_safe']}")
except ContentModerationError as e:
    print(f"‚ùå Image bloqu√©e: {e.reason}")
```

## üîß Int√©gration Frontend

### React Native / Expo

```typescript
const uploadClothingImage = async (imageUri: string) => {
  try {
    const formData = new FormData();
    formData.append("file", {
      uri: imageUri,
      type: "image/jpeg",
      name: "clothing.jpg",
    });

    const response = await fetch("http://localhost:8000/analyze", {
      method: "POST",
      body: formData,
    });

    if (response.status === 451) {
      // Contenu inappropri√© d√©tect√©
      const error = await response.json();
      Alert.alert(
        "Image refus√©e",
        "Cette image contient du contenu inappropri√©. Veuillez uploader une photo de v√™tement.",
        [{ text: "OK" }]
      );
      return null;
    }

    if (!response.ok) {
      throw new Error("Erreur lors de l'analyse");
    }

    const data = await response.json();
    return data;
  } catch (error) {
    console.error("Erreur:", error);
    throw error;
  }
};
```

### JavaScript / Fetch

```javascript
async function analyzeImage(file) {
  const formData = new FormData();
  formData.append("file", file);

  try {
    const response = await fetch("http://localhost:8000/analyze", {
      method: "POST",
      body: formData,
    });

    if (response.status === 451) {
      const error = await response.json();
      alert(`Image bloqu√©e: ${error.detail.reason}`);
      return null;
    }

    const data = await response.json();
    return data;
  } catch (error) {
    console.error("Erreur:", error);
    throw error;
  }
}
```

## ‚öôÔ∏è Configuration Personnalis√©e

### Ajuster le Seuil

Pour rendre la d√©tection plus stricte :

```python
# config.py
CONTENT_MODERATION_CONFIG = {
    "enabled": True,
    "nsfw_threshold": 0.4,  # Plus strict (40%)
    "block_nsfw": True,
}
```

Pour rendre la d√©tection plus permissive :

```python
# config.py
CONTENT_MODERATION_CONFIG = {
    "enabled": True,
    "nsfw_threshold": 0.75,  # Plus permissif (75%)
    "block_nsfw": True,
}
```

### D√©sactiver la Mod√©ration (D√©veloppement)

```python
# config.py
CONTENT_MODERATION_CONFIG = {
    "enabled": False,  # D√©sactiv√©
    "nsfw_threshold": 0.6,
    "block_nsfw": True,
}
```

## üìä M√©triques

Le syst√®me retourne des m√©triques de d√©tection :

```python
{
  "is_safe": false,
  "reasons": ["nudit√© d√©tect√©e"],
  "skin_percentage": 0.78,    # 78% de peau d√©tect√©e
  "brightness": 185.5,        # Luminosit√© moyenne
  "confidence": 0.78          # Confiance de la d√©tection
}
```

## üöÄ √âvolutions Futures

### Pour Production

- [ ] Mod√®le ML sp√©cialis√© (NudeNet, NSFW Detector)
- [ ] D√©tection de visages
- [ ] D√©tection de contenu violent
- [ ] Cache de mod√©ration (hash des images)
- [ ] Logging et analytics
- [ ] API de r√©vision manuelle

### Mod√®les ML Recommand√©s

1. **NudeNet** - D√©tection NSFW pr√©cise
2. **CLIP** - Analyse s√©mantique du contenu
3. **OpenAI Moderation API** - Service cloud
4. **Google Cloud Vision** - API de mod√©ration

## ‚ö†Ô∏è Limitations

### D√©tection Basique (MVP)

- Utilise une heuristique de couleur de peau
- Peut avoir des faux positifs/n√©gatifs
- Ne d√©tecte pas le contexte s√©mantique

### Faux Positifs Possibles

- Photos de maillots de bain sur mannequin
- Portraits en gros plan
- Images tr√®s claires/surexpos√©es

### Faux N√©gatifs Possibles

- Contenu censur√©/flout√©
- Images sombres
- Certains types de contenu inappropri√©

## üí° Recommandations

### Pour le MVP

‚úÖ La d√©tection actuelle est suffisante pour un MVP
‚úÖ Prot√®ge contre les cas d'abus √©vidents
‚úÖ Configurable et facile √† ajuster

### Pour la Production

‚ö†Ô∏è Envisager un mod√®le ML sp√©cialis√©
‚ö†Ô∏è Ajouter une r√©vision manuelle
‚ö†Ô∏è Impl√©menter un syst√®me de signalement utilisateur

## üìû Support

Pour toute question sur la mod√©ration :

1. V√©rifier la configuration dans `config.py`
2. Consulter les logs du serveur
3. Tester avec `test_moderation.py`
4. Ajuster le seuil `nsfw_threshold` si n√©cessaire
